@online{li_fei_fei,
  author = "Li Fei-Fei",
  title = "With spatial intelligence, AI will understand the real world",
  url  = "https://www.ted.com/talks/fei_fei_li_with_spatial_intelligence_ai_will_understand_the_real_world",
  keywords = "li fei-fei, ted talk, spatial intelligence, ai, real world",
  year = "2024"
}

@article{lecun_cnn,
    author = "Yan LeCun",
    title = "Backpropagation applied to handwritten zip code recognition",
    journal = "Neural Computation",
    volume = "1",
    pages = "541--551",
    year = "1989",
    DOI = "10.1162/neco.1989.1.4.541",
    keywords = "CNN"
}

@article{alexnet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet classification with deep convolutional neural networks},
    year = {2012},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
    booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
    pages = {1097–1105},
    numpages = {9},
    location = {Lake Tahoe, Nevada},
    series = {NIPS'12}
}

@article{vgg,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

@article{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@online{cnn_limits,
  author={Embedl},
  title={Vision Transformers vs CNNs at the Edge},
  url={https://www.embedl.com/knowledge/vision-transformers-vs-cnns-at-the-edge},
  keywords={CNN, disadvantage},
  year={2024}
}

@article{lenet,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}

@online{cnn_history,
  author={Daksh Bhatnagar},
  title={History of CNN and its impact in the field of Artificial Intelligence},
  url={https://medium.com/accredian/history-of-cnn-its-impact-in-the-field-of-artificial-intelligence-2b1efb7d99e5},
  keywords={CNN, history},
  year={2023}
}

@online{cnn_history_2,
  author={Avishek Biswas},
  title={The History of Convolutional Neural Networks for Image Classification (1989 - Today)},
  url={https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20},
  keywords={CNN, history},
  year={2024}
}

@article{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@article{hvc_pic,
author = {Roffo, Giorgio},
year = {2017},
month = {06},
title = {Ranking to Learn and Learning to Rank: On the Role of Ranking in Pattern Recognition Applications}
}

@online{human_visual_cortex,
  author={Zoumana Keita},
  title={An Introduction to Convolutional Neural Networks (CNNs)},
  url={https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns},
  keywords={CNN, human visual cortex},
  year={2023}
}

@online{conv,
  author={Yulia Gavrilova},
  title={Convolutional Neural Networks for Beginners},
  url={https://serokell.io/blog/introduction-to-convolutional-neural-networks},
  keywords={CNN, architecture},
  year={2021}
}

@online{cnn_arch,
  author={Pratham Modi},
  title={Convolutional Neural Networks for Dummies},
  url={https://medium.com/@prathammodi001/convolutional-neural-networks-for-dummies-a-step-by-step-cnn-tutorial-e68f464d608f},
  keywords={CNN, architecture},
  year={2023}
}

@online{padding,
  author={savyakhosla},
  title={CNN | Introduction to Padding},
  url={https://www.geeksforgeeks.org/cnn-introduction-to-padding/},
  keywords={CNN, architecture},
  year={2023}
}

@online{stride,
  author={deepai},
  title={Stride (Machine Learning)},
  url={https://deepai.org/machine-learning-glossary-and-terms/stride},
  keywords={CNN, architecture},
  year={2024}
}

@article{pooling,
author = {Yani, Muhamad and Irawan, S, and Setianingsih, Casi},
year = {2019},
month = {05},
pages = {012052},
title = {Application of Transfer Learning Using Convolutional Neural Network Method for Early Detection of Terry’s Nail},
volume = {1201},
journal = {Journal of Physics: Conference Series},
doi = {10.1088/1742-6596/1201/1/012052}
}

@online{perceptron,
  author={Sejai Jaiswai},
  title={Multilayer Perceptrons in Machine Learning: A Comprehensive Guide},
  url={https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning},
  keywords={CNN, architecture},
  year={2024}
}

@article{attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@book{rnn,
  author={Rumelhart, David E. and McClelland, James L.},
  booktitle={Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations}, 
  title={Learning Internal Representations by Error Propagation}, 
  year={1987},
  volume={},
  number={},
  pages={318-362},
  keywords={},
  doi={}}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@article{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}


@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@online{trans_motivation,
  author={Sunke},
  title={Brief history and motivation for the development of Transformer model},
  url={https://medium.com/@sunke1920/brief-history-and-motivation-for-the-development-of-transformer-model-920178ae4193},
  keywords={transformer},
  year={2024}
}

@online{rnn_downsides,
  author={Indrajitbarat},
  title={Recurrent Neural Networks (RNNs): Challenges and Limitations},
  url={https://medium.com/@indrajitbarat9/recurrent-neural-networks-rnns-challenges-and-limitations-4534b25a394c},
  keywords={transformer},
  year={2023}
}

@article{rnn_pic,
author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
year = {2017},
month = {10},
pages = {},
title = {Understanding Hidden Memories of Recurrent Neural Networks}
}

@online{lstms_explained0,
  author={Christopher Olah},
  title={Understanding LSTM Networks},
  url={https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  keywords={lstm},
  year={2015}
}

@online{lstms_explained,
  author={Ottavio Calzone},
  title={An Intuitive Explanation of LSTM},
  url={https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c},
  keywords={lstm},
  year={2022}
}

@online{trans_exp,
  author={Umar Jamil},
  title={Attention is all you need (Transformer) - Model explanation (including math), Inference and Training},
  url={https://youtu.be/bCz4OMemCcA?si=g056Geiv1Z-UkKo7},
  year={2023}
}

@online{attention_head,
  author={Abby Morgan},
  title={Explainable AI: Visualizing Attention in Transformers},
  url={https://www.comet.com/site/blog/explainable-ai-for-transformers/},
  year={2023}
}

@online{transformer,
  author={Umar Jamil},
  title={Attention is all you need (Transformer) - Model explanation (including math), Inference and Training},
  url={https://github.com/hkproj/transformer-from-scratch-notes},
  year={2023}
}

@online{batch_vs_layer,
  author={Florian June},
  title={BatchNorm and LayerNorm},
  url={https://medium.com/@florian_algo/batchnorm-and-layernorm-2637f46a998b#:~:text=Difference%20between%20Batch%20Normalization%20and,all%20features%20within%20each%20sample.},
  year={2023}
}

@article{vit,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

@online{vit_impl,
  author={Brian Pulfer},
  title={Vision Transformers from Scratch (PyTorch): A step-by-step guide},
  url={https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c},
  year={2022}
}

@online{gordic,
  author={Aleksa Gordic},
  title={Vision Transformer (ViT) - An image is worth 16x16 words | Paper Explained},
  url={https://youtu.be/j6kuz_NqkG0?si=oE9UTIuAGm2SLdm6},
  year={2020}
}

@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}