@online{li_fei_fei,
    author = "Li Fei-Fei",
    title = "With spatial intelligence, AI will understand the real world",
    url  = "https://www.ted.com/talks/fei_fei_li_with_spatial_intelligence_ai_will_understand_the_real_world",
    keywords = "li fei-fei, ted talk, spatial intelligence, ai, real world"
}

@article{lecun_cnn,
    author = "Yan LeCun",
    title = "Backpropagation applied to handwritten zip code recognition",
    journal = "Neural Computation",
    volume = "1",
    pages = "541--551",
    year = "1989",
    DOI = "10.1162/neco.1989.1.4.541",
    keywords = "CNN"
}

@article{alexnet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    title = {ImageNet classification with deep convolutional neural networks},
    year = {2012},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
    booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
    pages = {1097–1105},
    numpages = {9},
    location = {Lake Tahoe, Nevada},
    series = {NIPS'12}
}

@article{vgg,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

@article{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{lenet,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}

@article{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@online{human_visual_cortex,
  author={Zoumana Keita},
  title={An Introduction to Convolutional Neural Networks (CNNs)},
  url={https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns},
  keywords={CNN, human visual cortex}
}

@article{attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@book{rnn,
  author={Rumelhart, David E. and McClelland, James L.},
  booktitle={Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations}, 
  title={Learning Internal Representations by Error Propagation}, 
  year={1987},
  volume={},
  number={},
  pages={318-362},
  keywords={},
  doi={}}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@article{bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}


@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@online{lstms_explained,
  author={Ottavio Calzone},
  title={An Intuitive Explanation of LSTM},
  url={https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c},
  keywords={lstm}
}

@online{attention_head,
  url={https://www.comet.com/site/blog/explainable-ai-for-transformers/}
}

@online{transformer,
  author={Umar Jamil},
  title={
Attention is all you need (Transformer) - Model explanation (including math), Inference and Training},
  url={https://github.com/hkproj/transformer-from-scratch-notes}
}

@article{vit,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}